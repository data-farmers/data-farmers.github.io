{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: Principal Component Analysis\n",
    "subtitle: A dimensionality reduction through orthogonal transformation \t \n",
    "gh-repo: data-farmers/code/pca\n",
    "tags: [multidimensional-scaling, liner]\n",
    "author: Alessandra\n",
    "comments: true\n",
    "---\n",
    "\n",
    "Principal components analysis (PCA) is used when a simpler representation is desired\n",
    "for a set of intercorrelated variables. It is a method of transforming the original\n",
    "variables into new, uncorrelated variables. The new variables are called the\n",
    "principal components.\n",
    "\n",
    "## Purpose\n",
    "This analysis occurs when the dataset includes many variables displaying strong pairwise correlations, its main purposes are:\n",
    "* reduce the number of variables from n to k where k < n,\n",
    "* avoid loss of relevant information contained in the data.\n",
    "\n",
    "Each principal component is a linear combination of the original variables. One measure of the amount of information conveyed by each principal component is its variance. For this reason the reduction of dimensionality is attained by retaining, in statistical analysis,\n",
    "only the first components, i.e. those with larger variances, while the last components, i.e. the least informative (a variable with zero variance does not distinguish between the members of the population), are dropped. \n",
    "### Possible uses of principal components:\n",
    "1. Regression modelling: reduce the number of predictors and rely on a selection of principal components. This helps avoiding problems of multicollinearity\n",
    "2.  Test normality of variables: if the principal components are not normally distributed, neither are the original variables\n",
    "3.  In segmentation problems where one wishes to cluster individuals according to some variables: if these are too many they can be replaced by principal components\n",
    "4.  As an exploratory tool that allows for a better understanding of the relationships among variables <sup>[1](#myfootnote1)</sup>.\n",
    "\n",
    "\n",
    "![Sammon1](../img/pca/sammon1.png)\n",
    "\n",
    "\n",
    "![Sammon2](../img/sammon/sammon2.png)\n",
    "\n",
    "\n",
    "Images taken from [here](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV0910/henderson.pdf).\n",
    "\n",
    "## The algorithm\n",
    "\n",
    "As proposed by J. W. Sammon in 1969 [[1]](https://dl.acm.org/citation.cfm?id=1310727), Sammon Mapping is not actually an algorithm, strictly speaking. Sammon defined an error funcion,\n",
    "in order to evaluate how good a given projection is in preserving the original structure, by assessing the difference in the distance between each pair of points\n",
    "_i_ and _j_ in the higher and in the lower dimensionality space. Following his original proposal, the function error E is a function of the\n",
    "Euclidean distance, as follows:\n",
    "\n",
    "$$ E = \\frac{1}{ \\sum_{i<j}^{} {d_{ij}^{\\\\*}}}  \\sum_{i<j}{\\frac{(d_{ij}^{\\\\*} - d_{ij})^2}{d_{ij}^{\\\\*}}} $$\n",
    "\n",
    "where $ d_{ij}^{\\*} $ is the distance between _i_ and _j_ in the original space, and $ d_{ij} $ is the distance between _i_ and _j_ once mapped into the new, lower dimensionality space.\n",
    "\n",
    "Thus, the projection itself is a optimization problem: for each pair of points {_i_, _j_}, we only need to find a new distance which is as close as possible to the original one,\n",
    "where \"as close as possible\" means beneath a given threshold $ \\epsilon $.\n",
    "Sammon proposed a steepest gradient descent algorithm to minimize the error, a method easy to apply but poor in performances.\n",
    "In the following years several other approaches have been used, like simulated annealing [[2]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.5626&rep=rep1&type=pdf) and neural networks [[3]](https://link.springer.com/chapter/10.1007/978-3-540-71629-7_21). \n",
    "\n",
    "Sun et al. [[4]](https://www.sciencedirect.com/science/article/pii/S0020025511005561?via%3Dihub) found out that using the left/right Bregman divergence as distance in the error function E is a major improvement for the whole algorithm in terms of performance.\n",
    "\n",
    "By default, iterative algorithms for the Sammon error minimization problem don't start from the original spatial configuration of datapoints, but from the Principal Component Analysis, as it has been shown to lead to a faster convergence.\n",
    "\n",
    "\n",
    "## The code\n",
    "\n",
    "Unfortunately, it doesn't look Python has a good library that includes the Sammon Mapping among its functions.\n",
    "The following code leverages a function taken from Tom Pollard's [repository](https://github.com/tompollard/sammon). Thanks.\n",
    "\n",
    "\n",
    "\n",
    "Here we go: we have a pandas dataframe listing some properties on 199 labelled seeds. Our dataset includes 7 features for each observation, making it impossible to draw...unless we reduce it to 3 or fewer dimensions with Sammon Mapping.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sammon import sammon\n",
    "\n",
    "df = pd.read_csv('../dataset/seeds.csv')\n",
    "\n",
    "# sammon(...) wants a Matrix\n",
    "X = df.as_matrix(columns = df.columns[:7])\n",
    "\n",
    "# By default, sammon returns a 2-dim array and the error E\n",
    "[y, E] = sammon(X)\n",
    "```\n",
    "\n",
    "Sammon Mapping in Python is simple as that, once you have your dataset the only thing to do is to convert it into a matrix, to feed sammon.py method with. The result is a n-dimensional array (where n=2 by default) you can plot.\n",
    "\n",
    "![sammonplot0](../img/sammon/sammonplot0.png){:height=\"150%\" width=\"150%\"}\n",
    "\n",
    "Why we can't use the more popular Principal Component Analysis? Apparently there's little difference:\n",
    "\n",
    "![sammonplot1](../img/sammon/sammonplot1.png){:height=\"150%\" width=\"150%\"}\n",
    "\n",
    "This time Sammon Mapping has found a projection very similar to the one PCA found. Maybe, by chance, the best way to preserve mutual distances between data points looks very close to the best way to magnify their variance. On the other hand, don't forget that **sammon starts from the PCA configuration for its iterations**, and the default number of iterations (500) could be unsufficient to converge to some very different configuration with such a high dimensionality.\n",
    "\n",
    "Let's try with choosing 3 features to keep, as if our dataset was 3-dimensional.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sammon import sammon\n",
    "\n",
    "df = pd.read_csv('../dataset/seeds.csv')\n",
    "\n",
    "# sammon(...) wants a Matrix\n",
    "X = df.as_matrix(columns = ['Area', 'Perimeter', 'Compactness'])\n",
    "\n",
    "# By default, sammon returns a 2-dim array and the error E\n",
    "[y, E] = sammon(X)\n",
    "```\n",
    "\n",
    "As a reference, this plot shows the original spatial configuration of our datapoints, the result of Sammon Mapping and the Principal Component Analysis.\n",
    "\n",
    "![sammonplot2](../img/sammon/sammonplot2.png){:height=\"150%\" width=\"150%\"}\n",
    "\n",
    "Here it is more clear: Sammon Mapping's algorithm has found a way to represent the mutual distances between data points, while it doesn't account for a better displacement on the screen. PCA, instead, found the two components that best capture the variance across the dataset, and draw it accordingly.\n",
    "\n",
    "The full code of this demonstration is available at [HERE](https://github.com/data-farmers/code/sammon_mapping).\n",
    "\n",
    "{: .box-warning}\n",
    "**Warning:** Following Sammon's definition, if the distance between two points _i_ and _j_ is 0, the algorithm will try a division by 0.\n",
    "Check for your data points to be **non repeated**, or your code will crash!\n",
    "\n",
    "**See also:** [PCA vs Sammon Sampling](http://hisee.sourceforge.net/Examples/Boquet.html)\n",
    "\n",
    "\n",
    "<a name=\"myfootnote1\">1</a>: Rencher A.C. and Christensen W.F. *Methods of Multivariate Analysis*, 3rd ed., Wiley, 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
