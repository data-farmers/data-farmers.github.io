---
layout: post
title: Principal Component Analysis
subtitle: A dimensionality reduction through orthogonal transformation 	 
gh-repo: data-farmers/code/pca
tags: [multidimensional-scaling, liner]
author: Alessandra
comments: true
---

Principal components analysis (PCA) is used when a simpler representation is desired
for a set of intercorrelated variables. It is a method of transforming the original
variables into new, uncorrelated variables. The new variables are called the
principal components.

## Purpose
This analysis occurs when the dataset includes many variables displaying strong pairwise correlations, its main purposes are:
* reduce the number of variables from n to k where k < n,
* avoid loss of relevant information contained in the data.

Each principal component is a linear combination of the original variables. One measure of the amount of information conveyed by each principal component is its variance. For this reason the reduction of dimensionality is attained by retaining, in statistical analysis,
only the first components, i.e. those with larger variances, while the last components, i.e. the least informative (a variable with zero variance does not distinguish between the members of the population), are dropped. 

** Possible uses of principal components:**
1. Regression modelling: reduce the number of predictors and rely on a selection of principal components. This helps avoiding problems of multicollinearity
2.  Test normality of variables: if the principal components are not normally distributed, neither are the original variables
3.  In segmentation problems where one wishes to cluster individuals according to some variables: if these are too many they can be replaced by principal components
4.  As an exploratory tool that allows for a better understanding of the relationships among variables 
5. PCA for Data Visualization when the dimension are greater then 3<sup>[1](#myfootnote1)</sup>.

## The algorithm
![pca1](../img/pca/pca1.png)
Images taken from <sup>[1](#myfootnote1)</sup>.



## The code

```python
import pandas as pd

#dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

# load dataset into Pandas DataFrame
df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])

```
**PCA is effected by scale so you need to scale the features in your data.** If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data. [HERE](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py).


```python
from sklearn.preprocessing import StandardScaler
features = ['sepal length', 'sepal width', 'petal length', 'petal width']
# Separating out the features
x = df.loc[:, features].values
# Separating out the target
y = df.loc[:,['target']].values
# Standardizing the features
x = StandardScaler().fit_transform(x)
```


{: .box-warning}
**Warning:** 

**See also:** [PCA vs Sammon Sampling](http://hisee.sourceforge.net/Examples/Boquet.html)


<a name="myfootnote1">1</a>: Rencher A.C. and Christensen W.F. *Methods of Multivariate Analysis*, 3rd ed., Wiley, 2012

